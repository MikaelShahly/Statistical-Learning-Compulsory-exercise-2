---
title: "Compulsory Exercise 2: Title (give your project an informative title)"
author:
- Full name for group member \#1.
- Full name for group member \#2.
- Full name for group member \#3.
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output:
  # html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "In this project we are solving a classification problem with the Heart Failur prediction dataset. We implement 4 different categorization methodes to try to predict whether a person will suffer heart diseas or not, and our goal was to verify if boosted decision trees are the most accurate models. We utilized several python libraries to do hyperparameter optimization, data analysis and modeling. The classifications methodes we utilized was default decision trees, Random forest classification trees, Histogram-based boosted classification trees and logistical regression. We found that the Histogram-based boosted classification tree performed the best, with test-accuracy of 88.69%"
---

```{r setup, include=FALSE}
library(knitr)
# Feel free to change the setting as you see fit
knitr::opts_chunk$set(echo = TRUE,
                      tidy = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      strip.white = TRUE,
                      prompt = FALSE,
                      cache = TRUE,
                      size = "scriptsize",
                      fig.align = "center")

```

```{r, eval=TRUE, echo=FALSE}
library("knitr")
library("rmarkdown")
library("reticulate")
library("ggplot2")
library("gridExtra")
options(reticulate.plot.repr = TRUE)
```

<!--  Etc (load all packages needed). -->

## Introduction: Scope and purpose of your project

In this project we are using the Heart failure prediction dataset, given to us in the problem description, to try to create a model which can predict if a patient will suffer a heart disease or not. This is  a classification task, where where a value of 1 represents heart disease while 0 represents a healthy condition. We want to try to create the most accurate, lowest test error, model possible for us.

Since we know very little about heart disease and we are relatively inexperienced with statistical learning more specifically our goal is to test some different models and find which of them is the most accurate. We do not aspire to create the most readable model or necesearily the most robust model, we simply seek to check which of the tree models perform the best for this kind of dataset, and if decision trees outperform a MLR model. We want to test this as it has been stated that Boosted decision trees are the stat of the art models in statistical learning, and we want to check for ourselves if this is true or not. We will be doing hyperparameter tuning to try to increase the performance, but we will check our values against the default hyperparameters in case they outperform what we select. We will be working in python and utilizing several libraries which enable us to do the statistical modeling, hyperparameter tuning, plotting, etc. From this you could say that we are the audience and we are working on testing whether we can excecute what we have learned so far in statistical modeling, and weather some of the claims that has been proposed in the lecture are true or not.

Just for reference, you need to ahve the following python libraries installed on your computer to properly run the code:
- pandas
- numpy
- matplotlib
- seaborn
- optuna
- scikit-learn (This is the new library that has replaced sklearn which we have used, if you install this it should work)

Our source is the fedesoriano. (September 2021). Heart Failure Prediction Dataset. Retrieved [14.04.2024] from https://www.kaggle.com/fedesoriano/heart-failure-prediction, which is an aggregated dataset created from different datasets. We have the following breakdown:

Cleveland: 303 observations
Hungarian: 294 observations
Switzerland: 123 observations
Long Beach VA: 200 observations
Stalog (Heart) Data Set: 270 observations

Total observations: 1190, with 272 duplicated observations.

Which gives a final dataset of 918 unique observations.

The dataset contains 12 attributes per observation, this is the description given:

1. Age: age of patient [years]
2. Sex: sex of the patient [M: male, F: female]
3. ChestPainType: chest pain type [TA: typical angina, ATA: atypical angina, NAP: non-anginal pain, ASY: asymptomatic]
4. REstingBP: resting blood pressure [mm Hg]
5. Cholesterol: serum cholesterol [mm/dl]
6. FastingBS: fasting blood sugar [1: if fastingBS > 120 mg/dl, 0: otherwise]
7. RestingECG: resting electrocardiogram results  [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]
8. MaxHR: maximum heart rate achieved [Numeic value between 60 and 202]
9. ExerciseAngina: exercise-induced angina [Y: Yes, N: No]
10. Oldpeak: oldpeak = ST [Numeric value measured in depression]
11. ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]
12. HeartDisease: output class [1: heart disease, 0: normal]

## Descriptive data analysis/statistics

First of all we can note that our dataset seems to consist of mostly european and north american studies. This will introduce a bias into our data, in comparison to a dataset which contains samples from across the world. This makes our model more suited to similar populations, and it will probably be less accurate if we begin to apply it to African or Asian countries. It is very difficult to say how much of a difference, if any, it will make, but it is worth noting.  

```{python}
#importing necessary datasets and python packages 
import os
import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
import optuna 
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.linear_model import LogisticRegression

plt.style.use('ggplot')

data_df = pd.read_csv("heart.csv")
```

Lets start by getting a general overview of each column, if they are quantitative / qualitative and the shape of their distributions through a KDE.

```{python, fig.width = 10, fig.height = 18}
fig, axes = plt.subplots(nrows=4, ncols=3, dpi=800)
axes = axes.flatten()

for i, column in enumerate(data_df.columns):
    sns.histplot(data=data_df, x = column, kde=True, ax=axes[i], hue ="HeartDisease")
    axes[i].set_xlabel(column, fontsize = 15)
    axes[i].grid(True)

tight.layout()
plt.show()
```

```{python}
print(data_df.describe().iloc[:, :3])
print(data_df.describe().iloc[:, 3:])
```

```{python}
data_df.info()
```

*NOTE: NO MISSING DATA!* Checking the correlation between the target lable and predictors. We start by converting the qualitative predictors to quantitative by making dummy variables

```{python}
dummy_gender = pd.get_dummies(data_df['Sex'], prefix='sex', drop_first=True)
dummy_ChestPainType = pd.get_dummies(data_df['ChestPainType'], prefix='ChestPainType', drop_first=True)
dummy_RestingECG = pd.get_dummies(data_df['RestingECG'], prefix='RestingECG', drop_first=True)
dummy_ExerciseAngina = pd.get_dummies(data_df['ExerciseAngina'], prefix='ExerciseAngina', drop_first=True)
dummy_St_Slope = pd.get_dummies(data_df['ST_Slope'], prefix='ST_Slope', drop_first=True)


data_quantitative = pd.concat([data_df.drop(['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'], axis=1), dummy_gender, dummy_ChestPainType, dummy_RestingECG, dummy_ExerciseAngina, dummy_St_Slope], axis=1)
```

```{python}
print(data_quantitative.corr()[["HeartDisease"]].sort_values(by="HeartDisease"))
```

based on this, all the predictors have a relativly high correlation to the target lable, with ST_Slope_FLat and ST_Slope_UP being some of the most important. More in-depth plots of the most important predictors. Lets also make check the complete correlation matrix.

NOTE: ANOVA test for numerical features and CHI-SQUARED test for categorical features can also be performed

```{python,  fig.width = 22, fig.height = 24}
fig = plt.figure()
sns.heatmap(data_quantitative.corr(), annot = True, annot_kws={"size": 20})
plt.yticks(fontsize=28)
plt.xticks(fontsize=28)
plt.show()
```

Now lets quickly make some box plots to check for outliers. Lets also make some quick scatterplots to ensure that there are no strange patters that indicate duplicated values.

```{python, fig.width = 10, fig.height = 18}
fig, axes = plt.subplots(nrows=2, ncols=3, dpi=800)
axes = axes.flatten()

for i, column in enumerate(data_df.drop(["Sex", "RestingECG", "ExerciseAngina", "ST_Slope", "ChestPainType"], axis=1).columns):
    sns.boxplot(data=data_df, y = column, ax=axes[i], x ="HeartDisease")
    axes[i].set_xlabel(column, fontsize = 15)
    axes[i].grid(True)

tight.layout()
plt.show()

```

```{python, fig.width = 10, fig.height = 18}
fig, axes = plt.subplots(nrows=2, ncols=3, dpi=800)
axes = axes.flatten()

for i, column in enumerate(data_df.drop(["Sex", "RestingECG", "ExerciseAngina", "ST_Slope", "ChestPainType"], axis=1).columns):
    sns.violinplot(data=data_df, y = column, ax=axes[i], x ="HeartDisease")
    axes[i].set_xlabel(column, fontsize = 15)
    axes[i].grid(True)

tight.layout()
plt.show()
```

Something seems off about Cholesterol. Extremely many seem to have 0 here. This either indicates plain errors in data or that we have many missing values here that were filled in with 0-val (which is fully possible as it seems there is no missing values).

```{python, fig.width = 10, fig.height = 25}
fig, axes = plt.subplots(nrows=6, ncols=2, dpi=800)
axes = axes.flatten()

for i, column in enumerate(data_df.columns):
    sns.scatterplot(data=data_df, x = data_df.index, y = column, ax=axes[i], hue ="HeartDisease")
    axes[i].set_title(column)
    axes[i].set_ylabel("")
    axes[i].grid(True)

tight.layout()
plt.show()
```

Nothing immediately stands out other then 1 person having 0 in Resting BP, which is likely a data error. We also see the 0 value cholesterol people that also is likely an error.

### Manipulating given the dataset

Lets start by removing the strange cholesterol values.

```{python}
chol_zero = data_df[data_df["Cholesterol"] == 0]
data_df.loc[data_df["Cholesterol"] == 0, "Cholesterol"] = pd.NA
data_quantitative.loc[data_quantitative["Cholesterol"] == 0, "Cholesterol"] = pd.NA
```

this is alot of values. Either the entire col needs to be dropped the rows need to be dropped (which is not realistic). Lets analyze this data to check for trends. If not we can drop the col or attempt to impute the data by MICE

```{python, fig.width = 10, fig.height = 8}
fig, axes = plt.subplots(nrows=2, ncols=2, dpi=800)
axes = axes.flatten()

for i, column in enumerate(chol_zero.columns):
    sns.histplot(data=chol_zero, x = column, kde=True, ax=axes[i], hue ="HeartDisease")
    axes[i].set_title(column, fontsize = 15)
    axes[i].set_xlabel("")
    axes[i].grid(True)

tight.layout()
plt.show()
```

```{python}
print(data_quantitative.corr()[["Cholesterol"]].sort_values(by="Cholesterol"))
```

There are some correlations, so we could attempt a MICE imputation on the missing values. NOTE as we plan on mainly relying on decision trees we can just set these as missing values aswell.

Note: We have not handled outliers which will likely affect our results. Especially if we use a model which is sensitive to outliers that have a large leverage.

#### Imputation

Lets start by converting our dataframe to an R dataframe in order to perform MICE on the missing values.

```{r}
library("mice")
R_dataFrame <- as.data.frame(py$data_quantitative)

tempData <- mice(R_dataFrame,m=5,maxit=50,seed=500) #using default imputation technique
summary(tempData)
```

```{r}
imputed_R_dataFrame <- complete(tempData,1)

#lets convert back to pyhton
imputed_dataFrame <- r_to_py(imputed_R_dataFrame)


```

```{python}
imputed_dataFrame = r.imputed_R_dataFrame
fig = plt.figure()
sns.histplot(data = imputed_dataFrame, x = "Cholesterol", hue = "HeartDisease", kde=True)
plt.show()
```

This follows the expected Gaussian distribution. NOTE: forgot to remove target labels when imputing with MICE, so we likely included some data leakage to the feature. This can cause an increase in over fitting of the model.

### is this gaussian? it doesnt look symmetrical enough, maybe weibull?


#### Scaling

consider scaling data

.

## Methods

We will start by creating Decision Tree models (normal / Random Forest / Bagged / Boosted) as they can handle our missing values well and are also not very sensitive to outliers in predictor values (which is good, as we have not done a thorough analysis and handling of potential outliers).

```{python}
# Assuming X contains features and y contains labels/target variable
# Split data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(data_df.drop("HeartDisease", axis=1), data_df["HeartDisease"], test_size=0.2, random_state=42)

# Split data into training and test sets (80% train, 20% test) for hot-one encoded data
X_train_dummy, X_test_dummy, y_train_dummy, y_test_dummy = train_test_split(data_quantitative.drop("HeartDisease", axis=1), data_df["HeartDisease"], test_size=0.2, random_state=42)

# Split data into training and test sets (80% train, 20% test) for imputed data
X_train_imp, X_test_imp, y_train_imp, y_test_imp = train_test_split(imputed_dataFrame.drop("HeartDisease", axis=1), data_df["HeartDisease"], test_size=0.2, random_state=42)
```

The train / validation / test split will effect the bias and varience of models traied on the train data. Larger train data percentage implies less bias and higher varience. Lower train data percentage implies higher bias and lower variance.

#### 1) Decision Tree

lets create a basic decision tree using SKLearn default tree classifier. The default hyperparamaters are:\
(*\**, *criterion=['gini']{.underline}*, *splitter=['best']{.underline}*, *max_depth=[None]{.underline}*, *min_samples_split=[2]{.underline}*, *min_samples_leaf=[1]{.underline}*, *min_weight_fraction_leaf=[0.0]{.underline}*, *max_features=[None]{.underline}*, *random_state=[None]{.underline}*, *max_leaf_nodes=[None]{.underline}*, *min_impurity_decrease=[0.0]{.underline}*, *class_weight=[None]{.underline}*, *ccp_alpha=[0.0]{.underline}*, *monotonic_cst=[None]{.underline}*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/f07e0138b/sklearn/tree/_classes.py#L698)[¶](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier "Link to this definition").

Important hyperparamters to note are the splitting criterion, and stopping criteria such as: max_leaf_nodes, max_depth, min_impurity_decrease, as well as the pruning modifier ccp_alpha which is a complexity parameter when pruning. When pruning the subtree with the largest cost.complexity below our ccp_alpha value will be chosen.

These need to be correctly chosen in order to strike a good balance between bias / variance of model.

```{python}


def objective(trial, X_train, y_train):
    params = {
          "max_depth": trial.suggest_int('max_depth', 5, 15),
          "criterion": trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),
          "ccp_alpha": trial.suggest_float("ccp_alpha", 0.001, 0.1),
          "min_samples_split": trial.suggest_int("min_samples_split", 2, 10)
    }

    model = DecisionTreeClassifier(random_state=42, **params)
    cv_accuracy = cross_val_score(model, X_train_dummy, y_train_dummy, cv=8)
    
    return np.mean(cv_accuracy)

study = optuna.create_study(direction='maximize')
study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=60)
```

```{python}
print("Best trial value:", study.best_params, study.best_value)
```

```{python}
params = study.best_params

DT_model_tuned = DecisionTreeClassifier(random_state=42, **params)
DT_model_default = DecisionTreeClassifier(random_state=42)

DT_model_tuned.fit(X_train_dummy, y_train_dummy)
DT_model_default.fit(X_train_dummy, y_train_dummy)

y_pred_tuned = DT_model_tuned.predict(X_test_dummy)
y_pred_default = DT_model_default.predict(X_test_dummy)
```

```{python}


# Calculate confusion matrix
confusion_matrix_df = pd.DataFrame(data=confusion_matrix(y_test, y_pred_tuned), 
                                   index=["True Pos", "True Neg"], 
                                   columns=["Predicted Pos", "Predicted Neg"])

# Plot confusion matrix
fig = plt.figure()
sns.heatmap(confusion_matrix_df, annot=True)
plt.show()

```
```{python}
# Calculate accuracy and print
accuracy = accuracy_score(y_test, y_pred_tuned)
accuracy_default = accuracy_score(y_test, y_pred_default)

print(f"The accuracy of hyperparamater tuned is: {accuracy} and the accuracy of not hyperparamater tuned is: {accuracy_default}")
```

### 2) Random Forest

Lets now go through the same with a random forest model. The math behind why we should bag a tree with bootstrap samples is shown below: $var(\bar{Z}) = var{(1 / n)*\sum{Z_i}} = \frac{1}{n^2}*\sum{var{Z_i}} = \frac{\sigma^2}{n}$

with correlation the last part becomes: $\text{Var}\left(\sum_{i} X_i\right) = \sum_{i}\sum_{j} \text{Cov}(X_i, X_j)$ $\text{Var}\left(\sum_{i} X_i\right) = \sum_{i}\sum_{j} \rho_{ij} \cdot \sigma_i \cdot \sigma_j$ Random Forest therefore only split

For our hyperparamater tuning we have some special things to take into consideration for a Random Forest: 1) Likly there is no point in pruning the bagged trees as we likly introduce to much bias then 2) Adding more estimators will never hurt our accuracy as we are only reducing the varience more as shown by: $var(\bar{Z}) = var{(1 / n)*\sum{Z_i}} = \frac{1}{n^2}*\sum{var{Z_i}} = \frac{\sigma^2}{n}$ However for a large n the reduction will become negligible and only introduce unneccesary train time.

```{python}


def objective(trial, x_train, y_train):
    params = {
          "max_depth": trial.suggest_int('max_depth', 5, 15),
          "criterion": trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),
          "min_samples_split": trial.suggest_int("min_samples_split", 2, 10),
          "max_features":  trial.suggest_categorical("max_features", ['sqrt', 'log2']),
          "max_samples": trial.suggest_float("max_samples", 0.5, 1)
    }

    model = RandomForestClassifier(random_state=42, **params)
    cv_accuracy = cross_val_score(model, X_train_dummy, y_train_dummy, cv=5)
    
    return np.mean(cv_accuracy)

study = optuna.create_study(direction='maximize')
study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=200)
```

```{python}
print("Best trial value:", study.best_params, study.best_value)
```

```{python}
params_RF = {'max_depth': 15, 'criterion': 'gini', 'min_samples_split': 3, 'max_features': 'log2', 'max_samples': 0.8926729406142951}

RF_model_tuned = RandomForestClassifier(random_state=42, **params_RF, n_estimators = 500)
RF_model_default = RandomForestClassifier(random_state=42)

RF_model_tuned.fit(X_train_dummy, y_train_dummy)
RF_model_default.fit(X_train_dummy, y_train_dummy)

RFy_pred_tuned = RF_model_tuned.predict(X_test_dummy)
RFy_pred_default = RF_model_default.predict(X_test_dummy)
```

```{python}
# Calculate accuracy and print
accuracy = accuracy_score(y_test, RFy_pred_tuned)
accuracy_default = accuracy_score(y_test, RFy_pred_default)

print(f"The accuracy of hyperparamater tuned is: {accuracy} and the accuracy of not hyperparamater tuned is: {accuracy_default}")
```

#### Creating a analyze the effect of n_estimators for random forest

Just a quick plot to confirm the math of more estimators

```{python, echo = False}
test_accuracy_list = []
for i in range(1,300):
    RF_model = RF_model_tuned = RandomForestClassifier(random_state=42, n_estimators = i)
    RF_model.fit(X_train_dummy, y_train_dummy)
    RF_pred_y = RF_model.predict(X_test_dummy)
    accuracy_RF = accuracy_score(y_test, RF_pred_y)
    test_accuracy_list.append(accuracy_RF)
```

```{python}
fig = plt.figure()
plt.plot(range(1, 300), test_accuracy_list)
plt.show()
```

The results here clearly correlate to the theory given above, which is nice.

### 3) Boosted Decision Tree

We now trying to implement a boosted decision tree. This can be done from scratch using small decision trees (to ensure that we are creating an ensamble of weak learner, as the goal of boosting is primarily to decrease the bias of a weak learner and not to reduce the variance of a strong learner) and using a gradient boosting algorithm. Or we can import a finished implementation.

Here we choose to use the simple gradient boosted decision tree classifier from Scikit learn. Note; we will use the HistGradientBoostedClassifier as the normal GBM tree class in Scikit-Learn does not have an inherent way to handle missing values.

```{python}


def objective(trial, x_train, y_train):
    params = {
          "max_depth": trial.suggest_int('max_depth', 4, 6), #typical values
          "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.1),
          "max_iter": trial.suggest_int("max_iter", 100, 1000),
          "max_features": trial.suggest_categorical("max_features", [0.5, 0.8, 1.0]), #attempts to reduce varience more by randomizing features per split like random forest
          "l2_regularization": trial.suggest_float("l2_regularization", 0.001, 0.01)
    }

    model = HistGradientBoostingClassifier(random_state=42, **params)
    cv_accuracy = cross_val_score(model, X_train_dummy, y_train_dummy, cv=5)
    
    return np.mean(cv_accuracy)

study = optuna.create_study(direction='maximize')
study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=150)
```

```{python}
print("Best trial value:", study.best_params, study.best_value)
```

```{python}
params_GBM = {'max_depth': 4, 'learning_rate': 0.06526387443707683, 'max_iter': 143, 'max_features': 0.5, 'l2_regularization': 0.0035912380966771833}
GBM_model_tuned = HistGradientBoostingClassifier(random_state=42, **params_GBM)
GBM_model_default = HistGradientBoostingClassifier(random_state=42)

GBM_model_tuned.fit(X_train_dummy, y_train_dummy)
GBM_model_default.fit(X_train_dummy, y_train_dummy)

GBMy_pred_tuned = GBM_model_tuned.predict(X_test_dummy)
GBMy_pred_default = GBM_model_default.predict(X_test_dummy)
```

```{python}
# Calculate accuracy and print
accuracy = accuracy_score(y_test, GBMy_pred_tuned)
accuracy_default = accuracy_score(y_test, GBMy_pred_default)

print(f"The accuracy of hyperparamater tuned is: {accuracy} and the accuracy of not hyperparamater tuned is: {accuracy_default}")
```

Note: we are performing a sligh Ridge regularization above. However we have not scaled our dataset, which could lead to the Ridge Regression not performing as well as possible. Note, doing a stochastic GBM model could also be good here to try to reduce varience more.

#### 4) Logistic Regression

```{python}


def objective(trial, x, y):
    # Define the hyperparameters to optimize
    inverse_C = trial.suggest_loguniform('inverse_C', 1e-3, 1e4)
    # Calculate the regularization strength C from the inverse_C
    C = 1.0 / inverse_C
    model = LogisticRegression(random_state=42, C=C, max_iter = 1000)
    cv_accuracy = cross_val_score(model, X_train_imp, y_train_imp, cv=5)
    
    return np.mean(cv_accuracy)
  

study = optuna.create_study(direction='maximize')
study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=150)
```

```{python}
# Plot the optimization history
optuna_plot = optuna.visualization.plot_slice(study)
optuna_plot.show()
```

```{python}
print("Best trial value:", study.best_params, study.best_value)
```

```{python}
params_logReg = {'C': 1 / 4.290748352896671}
logReg_model_tuned = LogisticRegression(random_state=42, **params_logReg, max_iter = 1000)
logReg_model_default =LogisticRegression(random_state=42)

logReg_model_tuned.fit(X_train_imp, y_train_imp)
logReg_model_default.fit(X_train_imp, y_train_imp)

logRegy_pred_tuned = logReg_model_tuned.predict(X_test_imp)
logRegy_pred_default = logReg_model_default.predict(X_test_imp)
```

```{python}
# Calculate accuracy and print
accuracy = accuracy_score(y_test, logRegy_pred_tuned)
accuracy_default = accuracy_score(y_test, logRegy_pred_default)

print(f"The accuracy of hyperparamater tuned is: {accuracy} and the accuracy of not hyperparamater tuned is: {accuracy_default}")
```

As logistic regression in the Scikit package is based on gradient descent ( and not doing least squares to find best estimators as the statitical way of solving it does) we likly should have scaled the data before hand for a faster convergence.

We also get a better result with a relatively high regularization, which again correlates with the idea that reducing variance is one of the main factors here as we overall have a relatively small amount of data.

## Results and interpretation

<!-- What we need:  -->
<!-- 1 & 2: -->
<!-- - list of models and hyperparameters used -->
<!--     - use  -->
<!-- - barchart of accuracy for the different models (not doable right now) -->
<!-- - Discussion about which is the best performer -->
<!--   - Bias variance trade off -->
<!--   - computational cost -->
<!--   - flexibility -->
<!--   - raw performance -->
<!--   -was hyperparamer optimization worth it? time/performance increase -> most likely no -->
<!--   - Should we make a model that overpredicts heart failure? better to err on the safe side with      so dire consequences -->
<!--   - any predictors that are noteable? can we even get that information from the decision tree -->
<!--     models? -->

<!-- 3. -->
<!--   - Population bias -->
<!--   -says nothing about the time frame, only if its likely or not, can we use the model in time? or     does the predictors work to fast to test? -->

We made 8 different models in total. Two models per method, one where we used optuna to tune the hyperparameters, and one where we used the default values form the libraries. This is an overwiev of the different models:

Decision Tree:
  Best hyperparameters:
    'max_depth': 10
    'criterion': 'log_loss'
    'ccp_alpha': 0.0448131855142171
    'min_samples_split': 5
  
  Test accuracy:
    Tuned model: 0.842391304347826
    Default values: 0.8369565217391305

Random Forest Decision Tree:
  Best hyperparameters: 'max_depth': 14
  'criterion': 'gini'
  'min_samples_split': 7
  'max_features': 'log2'
  'max_samples': 0.8781738683197857
  
  Test accuracy:
    Tuned model:0.8695652173913043
    Default model: 0.8695652173913043

HistGradientBoostedClassifier model:
  Best hyperparameters: 
    'max_depth': 6
    'learning_rate': 0.017851462759686236
    'max_iter': 465
    'max_features': 0.5
    'l2_regularization': 0.009010122553197338
  
  Test accuracy:
    Tuned hyperparameters: 0.8858695652173914
    Default hyperparameters: 0.8804347826086957

Logistic Regression model:
  Best hyperparameters:
    'lambda': 4.195417061279373
  
  Test Accuracy:
    Tuned model: 0.8532608695652174
    Default model: 0.8695652173913043
    
It is important to note that the results may vary depending on which variables get selceted in the test and training dataset and by optuna.
    
As we can see the most accurate model is the histogram-based boosting classification tree (HistGradientBoostedClassifier). This is not a very surprising result as boosted decision trees are at the forefront of prediction accuracy when it comes to tabular data. However being an ensamble method, interpretation of the model is quite difficult as we are working with an ensamble of different trees. So to get the best model we are sacrificing readability. This also applies to Random Forest decision trees. The most readable would be the decision tree, if properly pruned, or the logistic regression model. For the decision tree model you could make a very readable flowchart, although it would be quite big without pruning. And for the logistical model we would simply check the coefficients to get a better understanding of which of the predictors matters the most.

The decision tree models offer us more flexibility than the logistical regression method as we are can create a huge variety in different models based on how many sections we create in the predictor space. Therefor controlling the variance with either pruning or by creating an ensamble of trees is important to avoid overfitting. That is one of the reasons the random forest and the boosted tree outperform the regular decision tree model.

For the hyperparameter optimization we can see that we managed to improve the accuracy for the most accurate model, which is the most important. Random Forest had no difference, which is suspiscious, but we could not find any mistake. Logistic regression on the other hand had better default values. This is also a bit weird, but not entirely unthinkable if optunas starting point for the optimization is different from the default values. That way we could end up not getting close to the optimal hyperparameters, which would open the possibility for the default values to be better. The default values are usually pretty good starting values for the parameters as they have been found empirically to perform pretty good. Not outperforming the default values implies that there is, probably a lot, more accuracy to gain in the model, but as we close in on 100% you get diminishing returns. The tuning with optuna introduced a heavy computational cost, as it was by far the optuna code chunks that required the most time to run. Determining if it was worth it or not all depends on how highly you value accuracy. In the case of predicting heart disease the costs of a false negative is so high that it is probably worth spending a lot of time and resources to improve the model even slight amounts. So if this was supposed to be used by doctors we could with a good consciense have spent even more time on fine tuning the hyperparameters.

On the note of cost of failure we can argue that, given that the models are supposed to be used by doctors for treatment of patients, we should try to make an oversensitive model. That means that we should probably over categorize the heart disease population even though it hurts our accuracy. That is because the cost of a false negative is much higher than the cost of a false positive. We would of course much rather over treat than under treat when under treatment can lead to death. That holds true until we have a capacity problem, which would cause the cost of a false positive to rise as we near the hospitals capacity. We could solve this by making different models depending on if capacity is a problem or not, or we could maybe change the response from heart disease to death or serious permanent harm caused by heart disease. That should allow us to perform cost-sensitive learning, although this seems to be outside of the scope of the course and our expertise.

Another aspect to note is that we are try to predict if a person will get heart disease a head of time, because that is what is valuable to know, and not whether they have it right now or not. To do this we are using data which only include if people have heart disease at the moment me observe them or not. This mean that we exclude some of the people who's predictors don't change but they will develop heart disease in the near future. And those people are closer to what we want to predict than who we use as positive responses right now. Thus our models is trained on something else than what we actually want to predict, and we have introduced a temporal bias into our models.

Also as we have mentioned before, there is a clear bias in our population compared to the global population. We have mostly people in the region of Europe and North America, which makes our models have a bias towards similar populations. How much of a bias we introduce by not including a more global population is unknown and impossible to quantify without another dataset or testing the models on a more global population, but however small there is likely to be some bias because of this.

## Summary

Tree model good uga booga.
